{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_feZb3i1I5qz"
   },
   "source": [
    "# 8章：ニューラルネット\n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbLnQEMeK-eC"
   },
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$X$と正解ラベルを並べた行列（ベクトル）$Y$を作成したい．\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\dots \\\\ \n",
    "  \\boldsymbol{x}_n \\\\ \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n \\times d},\n",
    "Y = \\begin{pmatrix} \n",
    "  y_1 \\\\ \n",
    "  y_2 \\\\ \n",
    "  \\dots \\\\ \n",
    "  y_n \\\\ \n",
    "\\end{pmatrix} \\in \\mathbb{N}^{n}\n",
    "$$\n",
    "\n",
    "\n",
    " ここで，$n$は学習データの事例数であり，$\\boldsymbol x_i \\in \\mathbb{R}^d$と$y_i \\in \\mathbb N$はそれぞれ，$i \\in \\{1, \\dots, n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．\n",
    " なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb N_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_i \\in \\mathbb N_{<4}$で表現できる．\n",
    " 以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
    "\n",
    " $i$番目の事例の特徴ベクトル$\\boldsymbol x_i$は，次式で求める．\n",
    "\n",
    " $$\\boldsymbol x_i = \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\mathrm{emb}(w_{i,t})$$\n",
    "\n",
    " ここで，$i$番目の事例は$T_i$個の（記事見出しの）単語列$(w_{i,1}, w_{i,2}, \\dots, w_{i,T_i})$から構成され，$\\mathrm{emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数は$d$）である．  \n",
    " すなわち，$i$番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$\\boldsymbol x_i$である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．$300$次元の単語ベクトルを用いたので，$d=300$である．  \n",
    " $i$番目の事例のラベル$y_i$は，次のように定義する．\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases}\n",
    "0 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「ビジネス」カテゴリの場合}) \\\\\n",
    "1 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「科学技術」カテゴリの場合}) \\\\\n",
    "2 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「エンターテイメント」カテゴリの場合}) \\\\\n",
    "3 & (\\mbox{記事}\\boldsymbol x_i\\mbox{が「健康」カテゴリの場合}) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
    "\n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "\n",
    " + 学習データの特徴量行列: $X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$\n",
    " + 学習データのラベルベクトル: $Y_{\\rm train} \\in \\mathbb{N}^{N_t}$\n",
    " + 検証データの特徴量行列: $X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$\n",
    " + 検証データのラベルベクトル: $Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$\n",
    " + 評価データの特徴量行列: $X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$\n",
    " + 評価データのラベルベクトル: $Y_{\\rm test} \\in \\mathbb{N}^{N_e}$\n",
    "\n",
    "なお，$N_t, N_v, N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの取り出し\n",
    "train = pd.read_table('data/train.txt')\n",
    "valid = pd.read_table('data/valid.txt')\n",
    "test = pd.read_table('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#タイトル列のみの抽出→特徴量の生成\n",
    "X_train = train['TITLE']\n",
    "X_valid = valid['TITLE']\n",
    "X_test = test['TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリ列のみの抽出→ラベルの生成\n",
    "Y_train = train['CATEGORY']\n",
    "Y_valid = valid['CATEGORY']\n",
    "Y_test = test['CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vecをダウンロード\n",
    "EMBEDDING_FILE = 'data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトル表現の生成関数を定義\n",
    "def create_vector(data):\n",
    "    result = []\n",
    "    for i in range(len(data)):\n",
    "        listdata = data[i].split(' ')\n",
    "        subresult = np.zeros(300)\n",
    "        number = 0\n",
    "        for j in range(len(listdata)):\n",
    "            try:\n",
    "                subresult += word_vectors[listdata[j]]\n",
    "                number += 1\n",
    "            except Exception as e: #単語埋め込み範囲外の語彙を見つけた時に例外処理\n",
    "                pass\n",
    "        if number == 0:\n",
    "            result.append(subresult)\n",
    "        else:\n",
    "            result.append(subresult/number)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量においてベクトル表現を生成\n",
    "X_train = create_vector(X_train)\n",
    "X_valid = create_vector(X_valid)\n",
    "X_test = create_vector(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ラベルにおいて特徴量を生成\n",
    "dct_int = {'b': 0, 't': 1, 'e': 2,'m': 3}\n",
    "Y_train = Y_train.replace(dct_int)\n",
    "Y_valid = Y_valid.replace(dct_int)\n",
    "Y_test = Y_test.replace(dct_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#型を合わせる\n",
    "X_train = torch.tensor(np.array(X_train)).float()\n",
    "X_valid = torch.tensor(np.array(X_valid)).float()\n",
    "X_test = torch.tensor(np.array(X_test)).float()\n",
    "\n",
    "Y_train = torch.tensor(Y_train.values)\n",
    "Y_valid = torch.tensor(Y_valid.values)\n",
    "Y_test = torch.tensor(Y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(X_train, 'data/X_train.pt')\n",
    "torch.save(X_valid, 'data/X_valid.pt')\n",
    "torch.save(X_test, 'data/X_test.pt')\n",
    "\n",
    "torch.save(Y_train, 'data/Y_train.pt')\n",
    "torch.save(Y_valid, 'data/Y_valid.pt')\n",
    "torch.save(Y_test, 'data/Y_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GCShFBPtZB0"
   },
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "\n",
    "$$ \n",
    "\\hat{y}_1=softmax(x_1W),\\\\\\hat{Y}=softmax(X_{[1:4]}W)\n",
    "$$\n",
    "\n",
    "\n",
    "ただし，$softmax$はソフトマックス関数，$X_{[1:4]}∈\\mathbb{R}^{4×d}$は特徴ベクトル$x_1$,$x_2$,$x_3$,$x_4$を縦に並べた行列である．\n",
    "\n",
    "$$\n",
    "X_{[1:4]}=\\begin{pmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "行列$W \\in \\mathbb{R}^{d \\times L}$は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．  \n",
    "なお，$\\hat{\\boldsymbol y_1} \\in \\mathbb{R}^L$は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトルである．\n",
    "同様に，$\\hat{Y} \\in \\mathbb{R}^{n \\times L}$は，学習データの事例$x_1, x_2, x_3, x_4$について，各カテゴリに属する確率を行列として表現している．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニューラルネットワーク構築\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(300, 4, bias=False)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = activation(self.layer1(input))\n",
    "        return output\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njupyter notebookでmodel(入力)を実行しようとすると\\n\\nKernel Restarting\\nThe kernel for masahiro_makino_chapter08.ipynb appears to have died. It will restart automatically.\\n\\nとエラーが出る。ちなみにサーバー上でPythonのスクリプトファイルを実行すればエラーが出ることなく実行できます。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#活性化関数の定義\n",
    "activation = nn.Softmax(dim=0)\n",
    "\n",
    "#Yの生成\n",
    "result = []\n",
    "#for i in range(4):\n",
    "    #result.append(model(X_train[i]))\n",
    "\n",
    "\n",
    "#Yのプリント\n",
    "#print(result)\n",
    "\n",
    "'''\n",
    "jupyter notebookでmodel(入力)を実行しようとすると\n",
    "\n",
    "Kernel Restarting\n",
    "The kernel for masahiro_makino_chapter08.ipynb appears to have died. It will restart automatically.\n",
    "\n",
    "とエラーが出る。ちなみにサーバー上でPythonのスクリプトファイルを実行すればエラーが出ることなく実行できます。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2AZ3dWHJqo9"
   },
   "source": [
    "## 72. 損失と勾配の計算\n",
    "\n",
    "学習データの事例x1\n",
    "と事例集合x1,x2,x3,x4\n",
    "に対して，クロスエントロピー損失と，行列W\n",
    "に対する勾配を計算せよ．なお，ある事例xi\n",
    "に対して損失は次式で計算される．\n",
    "\n",
    "$$\n",
    "l_i=−log[事例x_iがy_iに分類される確率]\n",
    "$$\n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uudzTu5mWLp1"
   },
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列W\n",
    "を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6uSmaljF8H0"
   },
   "source": [
    "## 74. 正解率の計測\n",
    "\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKnCbpspuexA"
   },
   "source": [
    "## 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnR6u561QW8I"
   },
   "source": [
    "## 76. チェックポイント\n",
    "\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpR29zZErLhv"
   },
   "source": [
    "## 77. ミニバッチ化\n",
    "\n",
    "問題76のコードを改変し，B\n",
    "事例ごとに損失・勾配を計算し，行列W\n",
    "の値を更新せよ（ミニバッチ化）．B\n",
    "の値を1,2,4,8,…\n",
    "と変化させながら，1エポックの学習に要する時間を比較せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 801 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: numpy in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: torch==1.11.0 in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from requests->torchvision) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from requests->torchvision) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/makino/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.X = X_train # 入力\n",
    "        self.Y = Y_train # 出力\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) # データ数を返す\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index番目の入出力ペアを返す\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数の定義\n",
    "loss_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "#最適化手法の決定\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()  # 重みとバイアスの更新で内部的に使用するデータをリセット\n",
    "        outputs = model(data[0])  # 手順1：ニューラルネットワークにデータを入力\n",
    "        loss = loss_entropy(outputs, data[1])  # 手順2：正解ラベルとの比較\n",
    "        loss.backward()  # 手順3-1：誤差逆伝播\n",
    "        optimizer.step()  # 手順3-2：重みとバイアスの更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []\n",
    "epoch_list = []\n",
    "for i in range(math.log2(len(X_train))):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=pow(2, i), shuffle=False)\n",
    "    start_time = time.perf_counter()\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()  # 重みとバイアスの更新で内部的に使用するデータをリセット\n",
    "        outputs = model(data[0])  # 手順1：ニューラルネットワークにデータを入力\n",
    "        loss = loss_entropy(outputs, data[1])  # 手順2：正解ラベルとの比較\n",
    "        loss.backward()  # 手順3-1：誤差逆伝播\n",
    "        optimizer.step()  # 手順3-2：重みとバイアスの更新\n",
    "    end_time = time.perf_counter()    \n",
    "    epoch_list.append(pow(2, i))\n",
    "    time_list.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv4gAVHc9Zkj"
   },
   "source": [
    "## 78. GPU上での学習\n",
    "\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pf9znncKUyM"
   },
   "source": [
    "## 79. 多層ニューラルネットワーク\n",
    "\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMweW7nY9vJqPb6nkp1XFDD",
   "collapsed_sections": [],
   "name": "Chapter8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
